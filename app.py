# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RZtvgk6cwkpDjdLI6w8Ikl8RI385iopP
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd


file_path = '/content/drive/MyDrive/Aurum AI ML/aurum_recommendation_data.csv'

df = pd.read_csv(file_path)

df.head()

print(df.head(5))

from sklearn.preprocessing import LabelEncoder

# Create a copy of the original DataFrame to preserve it
df_prepared = df.copy()

# Label Encode the categorical columns
categorical_cols = ["Skill_Required", "Job_Location", "Subcontractor_Name", "Preference_Match"]
label_encoders = {}

for col in categorical_cols:
    le = LabelEncoder()
    df_prepared[col] = le.fit_transform(df_prepared[col])
    label_encoders[col] = le

# Convert Job Outcome to rewards for reinforcement learning
df_prepared["Job_Outcome"] = df["Job_Outcome"].map({"Success": 1, "Failed": -1})

df_prepared.drop(columns=["Job_ID", "Subcontractor_Skills"], inplace=True)

df_prepared.head()

import numpy as np
from collections import defaultdict

# Define features to include in state
state_features = ['Skill_Required', 'Job_Location', 'Distance_km', 'Preference_Match', 'Job_Duration_hrs', 'Experience_Level_Yrs']
action_feature = 'Subcontractor_Name'
reward_feature = 'Job_Outcome'

# Create state-action-reward tuples
state_action_rewards = []
for i, row in df_prepared.iterrows():
    state = tuple(row[feat] for feat in state_features)
    action = row[action_feature]
    reward = row[reward_feature]
    state_action_rewards.append((state, action, reward))

# Initialize Q-table
q_table = defaultdict(lambda: np.zeros(len(df_prepared[action_feature].unique())))

# Parameters
alpha = 0.1  # learning rate
gamma = 0.9  # discount factor
epsilon = 0.1  # exploration rate
num_episodes = 1000

# Unique actions (subcontractors)
actions = df_prepared[action_feature].unique()

# Q-learning loop
for episode in range(num_episodes):
    for state, action, reward in state_action_rewards:
        action_idx = np.where(actions == action)[0][0]

        # Choose best action for next state (simplified since we’re not simulating steps)
        max_future_q = np.max(q_table[state])

        # Update Q-value
        current_q = q_table[state][action_idx]
        new_q = current_q + alpha * (reward + gamma * max_future_q - current_q)
        q_table[state][action_idx] = new_q

"""This code is training a Q-learning model using historical job data. It builds a table (q_table) that learns how well each subcontractor (action) performs under specific job conditions (state). Based on the past job outcome (+1 for success, -1 for failure), the Q-values are updated over multiple episodes to guide future recommendations."""

import numpy as np
from scipy.spatial.distance import cdist

def recommend_closest_match(input_state, q_table, actions):
    input_state = np.array(input_state).reshape(1, -1)

    # Convert Q-table keys to array for distance calculation
    q_keys = np.array(list(q_table.keys()))

    if len(q_keys) == 0:
        return "Q-table is empty"

    # Compute distance between input and each known state
    distances = cdist(input_state, q_keys, metric='euclidean')

    closest_idx = np.argmin(distances)
    closest_state = tuple(q_keys[closest_idx])

    best_action_index = np.argmax(q_table[closest_state])
    return actions[best_action_index]

"""This function is used to recommend the best subcontractor for a new job based on past data. It takes the details of the new job (like required skill, location, distance, etc.) and compares them to previous job situations stored in the Q-table. To find the most similar past job, it uses a method called Euclidean distance, which measures how close the new job is to past ones. Once the closest match is found, the function checks which subcontractor had the best performance (highest Q-value) in that situation and recommends them for the new job. This helps the system make smart decisions based on what worked well in the past."""

# New job input (real-world format)
new_job = {
    "Skill_Required": "Event Security",
    "Job_Location": "Sydney",
    "Distance_km": 7,
    "Preference_Match": "Yes",
    "Job_Duration_hrs": 3,
    "Experience_Level_Yrs": 4
}

# Encode the categorical features
input_encoded = [
    label_encoders['Skill_Required'].transform([new_job['Skill_Required']])[0],
    label_encoders['Job_Location'].transform([new_job['Job_Location']])[0],
    label_encoders['Preference_Match'].transform([new_job['Preference_Match']])[0],
]

# Final state for Q-learning (no normalization)
new_job_state = [
    input_encoded[0],                   # Skill_Required
    input_encoded[1],                   # Job_Location
    new_job['Distance_km'],             # Distance_km (raw)
    input_encoded[2],                   # Preference_Match
    new_job['Job_Duration_hrs'],        # Job_Duration_hrs (raw)
    new_job['Experience_Level_Yrs']     # Experience_Level_Yrs (raw)
]

"""This part of the code prepares a new job request so the model can find the best subcontractor for it. First, it defines the job's details like the skill required, location, distance, whether the subcontractor preferences match, job duration, and experience needed. Then, it converts the text-based inputs (like "Event Security" or "Yes") into numerical form using label encoders trained on previous data. Finally, it combines both the encoded and raw numerical values into a single list called new_job_state, which is the format required by the model to compare this job with past jobs and recommend the most suitable subcontractor."""

recommended = recommend_closest_match(new_job_state, q_table, actions)
print("Recommended Subcontractor (encoded):", recommended)

# Decode the subcontractor name using the label encoder
decoded_name = label_encoders["Subcontractor_Name"].inverse_transform([recommended])[0]
print("Final Recommended Subcontractor:", decoded_name)

# Step 1: Data Preparation

import pandas as pd
from sklearn.preprocessing import LabelEncoder, MinMaxScaler

# Load the dataset
df = pd.read_csv('/content/drive/MyDrive/Aurum AI ML/aurum_recommendation_data.csv')  # Adjust path if needed

# Create a copy for processing
df_prepared = df.copy()

# Step 1: Encode categorical columns
categorical_cols = ["Skill_Required", "Job_Location", "Subcontractor_Name", "Preference_Match"]
label_encoders = {}

for col in categorical_cols:
    le = LabelEncoder()
    df_prepared[col] = le.fit_transform(df_prepared[col])
    label_encoders[col] = le  # Save encoders for decoding later

# Step 2: Convert Job_Outcome to numerical rewards for Q-learning
df_prepared["Job_Outcome"] = df["Job_Outcome"].map({"Success": 1, "Failed": -1})

# Step 3: Drop irrelevant columns
df_prepared.drop(columns=["Job_ID", "Subcontractor_Skills"], inplace=True)

# Step 4: Normalize numeric features
scaler = MinMaxScaler()
numerical_cols = ["Distance_km", "Job_Duration_hrs", "Experience_Level_Yrs"]
df_prepared[numerical_cols] = scaler.fit_transform(df_prepared[numerical_cols])

# Final output
print("✅ Data preparation complete. Preview:")
print(df_prepared.head())

import numpy as np
from collections import defaultdict
import random

# Q-learning parameters
alpha = 0.1        # learning rate
gamma = 0.9        # discount factor
epsilon = 0.2      # exploration rate
episodes = 1000    # number of training iterations

# Prepare input and output data
states = df_prepared.drop(columns=["Job_Outcome", "Subcontractor_Name"]).values.astype(float)  # only feature inputs
rewards = df_prepared["Job_Outcome"].values.astype(int)  # success = 1, failed = -1

# Define actions: unique subcontractors (still encoded)
actions = df_prepared["Subcontractor_Name"].unique()
q_table = defaultdict(lambda: np.zeros(len(actions)))

# Q-learning training loop
for ep in range(episodes):
    for i in range(len(states)):
        state = tuple(states[i])
        reward = rewards[i]

        # Choose an action (exploration vs exploitation)
        if np.random.rand() < epsilon:
            action_idx = np.random.randint(0, len(actions))
        else:
            action_idx = np.argmax(q_table[state])

        # One-step update (no next state)
        next_state = state
        q_old = q_table[state][action_idx]
        q_next_max = np.max(q_table[next_state])

        # Q-value update
        q_table[state][action_idx] = q_old + alpha * (reward + gamma * q_next_max - q_old)

# Summary output
total_learned_states = len(q_table)
print(f"Q-learning training complete. Total unique states learned: {total_learned_states}")

"""This code block is responsible for training the Q-learning algorithm, which helps us learn which subcontractor is best suited for different types of job requirements. It starts by defining the learning parameters: the learning rate (`alpha`) determines how quickly the model updates its understanding; the discount factor (`gamma`) controls how much future rewards are considered; and the exploration rate (`epsilon`) decides whether the model should try new subcontractors or stick with what it already knows. Then, it prepares the input data by separating the features (job details) from the target (job outcome), and it identifies the list of possible subcontractors (actions). In the training loop, the model goes through each job entry and updates the Q-table — a record of how effective each subcontractor has been in different job situations. It does this over 1000 episodes to ensure strong learning. The Q-table grows with knowledge of which subcontractor performed well or poorly based on past job outcomes (success or failure). By the end, the model is equipped with a learned set of preferences for recommending subcontractors for new tasks based on past patterns.

"""

new_job_state = [3, 4, 0.2068965517241379, 1, 0.2857142857142857, 0.3333333333333333]

import numpy as np

# Example new job input (make sure the values match encoding and scaling from training)
new_job = {
    'Skill_Required': 'Event Security',
    'Job_Location': 'Sydney',
    'Distance_km': 7,
    'Preference_Match': 'Yes',
    'Job_Duration_hrs': 3,
    'Experience_Level_Yrs': 4
}

# Step 3.1: Encode categorical inputs using saved label encoders
input_encoded = [
    label_encoders['Skill_Required'].transform([new_job['Skill_Required']])[0],
    label_encoders['Job_Location'].transform([new_job['Job_Location']])[0],
    label_encoders['Preference_Match'].transform([new_job['Preference_Match']])[0]
]

# Step 3.2: Normalize numerical inputs using the same scaler
numerical_input = np.array([[new_job['Distance_km'], new_job['Job_Duration_hrs'], new_job['Experience_Level_Yrs']]])
normalized_input = scaler.transform(numerical_input)[0]

# Step 3.3: Concatenate encoded + normalized to create the new state
new_job_state = [
    input_encoded[0],      # Skill_Required
    input_encoded[1],      # Job_Location
    normalized_input[0],   # Distance_km
    input_encoded[2],      # Preference_Match
    normalized_input[1],   # Job_Duration_hrs
    normalized_input[2]    # Experience_Level_Yrs
]

new_job_state

from scipy.spatial.distance import cdist
import numpy as np

# Step 5: Define the correct matching function
def recommend_closest_match(input_state, q_table, actions):
    # Ensure input is a 2D array
    input_state = np.array(input_state).reshape(1, -1)

    # Extract and convert Q-table keys to proper 2D array (list of lists)
    q_keys_list = [list(key) for key in q_table.keys()]
    q_keys = np.array(q_keys_list)

    # Check for matching number of columns
    if q_keys.shape[1] != input_state.shape[1]:
        raise ValueError(f"Shape mismatch: input_state has {input_state.shape[1]} features, but q_keys has {q_keys.shape[1]}")

    # Compute distances and get closest match
    distances = cdist(input_state, q_keys, metric='euclidean')
    closest_idx = np.argmin(distances)
    closest_state = tuple(q_keys[closest_idx])

    # Return best subcontractor (action)
    best_action_index = np.argmax(q_table[closest_state])
    return actions[best_action_index]

"""

This function takes a new job's information (called `input_state`) and compares it to all the past job situations stored in the Q-table. It finds the most similar past job (using Euclidean distance) and looks up which subcontractor (action) performed best in that situation. It then recommends that subcontractor as the best match for the new job. This helps the model give smart suggestions even when the exact job scenario hasn't been seen before.
"""

# Convert DataFrame to NumPy array
data = df_prepared.values
states = data[:, :-1].astype(float)   # All features except Job_Outcome
rewards = data[:, -1].astype(int)     # Last column is Job_Outcome

# Example: New job details (human-readable)
new_job = {
    'Skill_Required': 'Event Security',
    'Job_Location': 'Sydney',
    'Distance_km': 7,
    'Preference_Match': 'Yes',
    'Job_Duration_hrs': 3,
    'Experience_Level_Yrs': 4
}

# Step 6.1: Encode categorical values using saved label encoders
input_encoded = [
    label_encoders['Skill_Required'].transform([new_job['Skill_Required']])[0],
    label_encoders['Job_Location'].transform([new_job['Job_Location']])[0],
    label_encoders['Preference_Match'].transform([new_job['Preference_Match']])[0]
]

# Step 6.2: Normalize numeric values using saved scaler
numeric_input = np.array([[new_job['Distance_km'], new_job['Job_Duration_hrs'], new_job['Experience_Level_Yrs']]])
normalized = scaler.transform(numeric_input)[0]

# Step 6.3: Combine all features to create final input state
new_job_state = tuple([
    input_encoded[0],        # Skill_Required
    input_encoded[1],        # Job_Location
    normalized[0],           # Distance_km
    input_encoded[2],        # Preference_Match
    normalized[1],           # Job_Duration_hrs
    normalized[2]            # Experience_Level_Yrs
])

# Check if exact state exists in Q-table
if new_job_state in q_table:
    recommended_idx = np.argmax(q_table[new_job_state])
else:
    # Approximate matching using closest state (Euclidean)
    from scipy.spatial.distance import cdist
    q_keys = np.array(list(q_table.keys()))

    distances = cdist([new_job_state], q_keys, metric='euclidean')
    closest_idx = np.argmin(distances)
    closest_state = tuple(q_keys[closest_idx])

    recommended_idx = np.argmax(q_table[closest_state])

# Step 8: Decode subcontractor name from index
subcontractor_decoder = label_encoders['Subcontractor_Name']
recommended_name = subcontractor_decoder.inverse_transform([recommended_idx])[0]

# Final Output
print("✅ Recommended Subcontractor:", recommended_name)

print(df.shape)

# Step 9: Recommend Top 3 Subcontractors (Most Similar State)
def recommend_top_n_matches(input_state, q_table, actions, n=3):
    input_state = np.array(input_state).reshape(1, -1)

    # Prepare Q-table states
    q_keys_list = [list(k) for k in q_table.keys()]
    q_keys = np.array(q_keys_list)

    # Compute distances
    distances = cdist(input_state, q_keys, metric='euclidean')
    closest_idx = np.argmin(distances)
    closest_state = tuple(q_keys[closest_idx])

    # Get Q-values for closest state
    q_values = q_table[closest_state]

    # Get top N action indices
    top_n_indices = np.argsort(q_values)[::-1][:n]

    # Decode subcontractor names
    top_subcontractors = label_encoders["Subcontractor_Name"].inverse_transform(top_n_indices)

    # Create result with Q-values
    results = [(name, q_values[i]) for name, i in zip(top_subcontractors, top_n_indices)]
    return results

# Run the top-N recommendation function
top_3 = recommend_top_n_matches(new_job_state, q_table, actions, n=3)

# Display output
print("✅ Top 3 Recommended Subcontractors:")
for name, score in top_3:
    print(f"- {name} (Q-Score: {round(score, 4)})")

"""This code finds the top 3 best subcontractors for a new job request. It compares the new job's conditions (like skill, location, distance, etc.) to previous jobs stored in the Q-table. It looks for the most similar past situation and retrieves the scores (Q-values) for all subcontractors. Then, it picks the top 3 subcontractors with the highest scores—these are the ones most likely to do a successful job based on past learning."""

from textblob import TextBlob
import numpy as np

# --- Ask user for ratings in 5 categories ---
print("\n📝 Please rate the subcontractor from 1 (Very Poor) to 5 (Excellent):")

categories = ["Punctuality", "Communication", "Skill Fit", "Professionalism", "Overall Satisfaction"]
ratings = []

for category in categories:
    while True:
        try:
            score = int(input(f"{category}: "))
            if 1 <= score <= 5:
                ratings.append(score)
                break
            else:
                print("⚠️ Please enter a number between 1 and 5.")
        except:
            print("⚠️ Invalid input. Please enter a number.")

# --- Ask for additional comments ---
feedback_text = input("\n💬 Additional comments (optional): ")
blob = TextBlob(feedback_text)
sentiment_score = blob.sentiment.polarity

# Average rating (out of 5)
avg_rating = sum(ratings) / len(ratings)

# Normalize it to range [-1, 1] → (for Q-learning reward)
rating_reward = (avg_rating - 3) / 2  # 3 → neutral midpoint

# Combine with sentiment score if available
if feedback_text.strip():
    final_reward = (rating_reward + sentiment_score) / 2
else:
    final_reward = rating_reward

# Clamp reward to range [-1, 1]
final_reward = max(min(final_reward, 1), -1)

# Get the selected action index
selected_idx = np.argmax(q_table[new_job_state])

# Get current Q-value
current_q = q_table[new_job_state][selected_idx]
max_future_q = np.max(q_table[new_job_state])

# Update Q-table
q_table[new_job_state][selected_idx] = current_q + alpha * (final_reward + gamma * max_future_q - current_q)

# Interpret final result
print(f"\n🧠 Feedback reward computed: {round(final_reward, 3)}")
if final_reward < -0.2:
    print("⚠️ Negative experience detected. Finding better subcontractors...")
    q_values = q_table[new_job_state].copy()
    q_values[selected_idx] = -np.inf  # exclude current

    alt_indices = np.argsort(q_values)[::-1][:2]
    alternate_names = label_encoders["Subcontractor_Name"].inverse_transform(alt_indices)

    print("\n🔁 Suggested Alternate Subcontractors:")
    for i, idx in enumerate(alt_indices):
        print(f"- {alternate_names[i]} (Q-Score: {round(q_values[idx], 4)})")
else:
    print("✅ Thank you! Feedback received and Q-table successfully updated.")